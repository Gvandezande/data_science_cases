{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Linear Regression Case\n",
    "Oefening Data Scientist \n",
    "Geert Vandezande\n",
    "\n",
    "Doel:\n",
    "- Supervised Learning toepassen\n",
    "- EDA uitvoeren op een dataset\n",
    "- Lineair Regression toepassen op de data: target is beter doen dan r² = 80% nauwkeurigheid die in de meeste uitwerkingen zit...\n",
    "- en nadien andere vormen van regressie toepassen, doel is om r² boven de 90% te krijgen\n",
    "\n",
    "==> Resultaat: r² = 90  --- yes, we did it....\n",
    "\n",
    "\n",
    "Extra:\n",
    "- er wordt logging voorzien voor en na de belangrijke stappen (zie LinReg_logging.log). Hiermee kunnen de stappen en de resultaten opgevolgd worden\n",
    "- we hebben een aantal herbruikbare code-blokken in een functie gestoken\n",
    "- een aparta class gemaakt voor BinaryValueEncoders\n",
    "\n",
    "Dataset: \n",
    "- More info: see kaggle https://www.kaggle.com/datasets/mirichoi0218/insurance/data\n",
    "\n",
    "\n",
    "Metadata :\n",
    "- age: age of primary beneficiary\n",
    "- sex: insurance contractor gender, female, male\n",
    "- bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n",
    "- children: Number of children covered by health insurance / Number of dependents\n",
    "- smoker: Smoking\n",
    "- region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
    "- charges: Individual medical costs billed by health insurance\n",
    "\n",
    "\n",
    "Volgorde van activiteiten in deze notebook: (cfr Datacamp \"preparing data for modelling)\n",
    "- data inlezen\n",
    "- data bekijken, visueel en numerisch\n",
    "- data summarizen via summarytools \n",
    "- missing en duplicated data oplossen \n",
    "- incorrect types controleren\n",
    "- numerische waarde standardizeren\n",
    "- categorische varaiabelen processen\n",
    "- feature engineering\n",
    "- linear Regression toepassen (ook Ridge en Lasso)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import van de diverse modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Machine learning algorithm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import  mean_absolute_percentage_error\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# system utils\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from colorama import Fore, Back, Style\n",
    "import sys\n",
    "import os\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra code snippits die doorheen de notebook gebruikt worden:\n",
    "\n",
    "save_fig: na generatie van een image kan de image naar file geschreven worden in de images/.. directory. Geef steeds een zinvolle naam\n",
    "\n",
    "read_JSON: om eenvoudig een JSON in te lezen\n",
    "\n",
    "log_info:\n",
    "- logging functie om doorheen de notebooks de status naar file te kunnen schrijven. \n",
    "- de logstatements worden tijdens de uitvoering van de code bewaard in een list. Die kan tussentijds naar het scherm geprint worden of naar een file\n",
    "- log_info_write_to_file: schrijf de loginformatie naar file \n",
    "- log_info_print_on_screen: print alle loginfo naar het scherm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enkele extra code snippets gebruikt doorheen de oefening\n",
    "\n",
    "plot_graphs = False\n",
    "\n",
    "# schrijf een visual naar file\n",
    "IMAGES_PATH = Path() / \"images\" \n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\n",
    "# functies om te loggen naar file\n",
    "# Lezen van de JSON-file\n",
    "log_info_lijst = []\n",
    "\n",
    "def read_JSON(file_path_read):\n",
    "    with open(file_path_read, 'r') as file:\n",
    "        files_from_json = json.load(file)\n",
    "    return files_from_json\n",
    "\n",
    "def log(log_code=\"INFO\", boodschap=\"euh geen boodschap????\"):\n",
    "    global log_info_lijst\n",
    "    now = datetime.datetime.now()\n",
    "    formatted_date = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_message = f\"{Style.RESET_ALL}{formatted_date} : {log_code} : {boodschap}\"\n",
    "    log_info_lijst.append(log_message)\n",
    "    print(log_message)\n",
    "    return\n",
    "\n",
    "def log_info(boodschap):\n",
    "    log(\"Info\",boodschap)\n",
    "\n",
    "def log_info_write_to_file(filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for string in log_info_lijst:\n",
    "            file.write(string + '\\n')  # Voeg een nieuwe regel toe na elke string\n",
    "    return\n",
    "\n",
    "def log_info_print_on_screen():\n",
    "    for boodschap in log_info_lijst:\n",
    "        print(boodschap)    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak een boxplot van een kolommen\n",
    "# df_num_col is een list van de kolomnamen die geplot worden\n",
    "\n",
    "def plot_boxplot(df, df_col, filenaam):\n",
    "    if plot_graphs:\n",
    "        # boxplot van de numerische waarden\n",
    "        sns.set_theme(style=\"whitegrid\", palette=\"bright\")\n",
    "        plt.figure(figsize=(15, 15)) \n",
    "        for i, col in enumerate(df_col):\n",
    "            plt.subplot(len(df_col), 2, 2 * i + 1)\n",
    "            sns.boxplot(x=df[col], orient='h', linewidth=1.5)\n",
    "            plt.title(f\"Boxplot of {col}\", fontsize=12, fontweight=\"bold\")\n",
    "            plt.xlabel(col, fontsize=10)\n",
    "\n",
    "            plt.subplot(len(df_col), 2, 2 * i + 2)\n",
    "            sns.histplot(df[col], kde=True,  linewidth=1)\n",
    "            plt.title(f\"Distribution Plot of {col}\", fontsize=12, fontweight=\"bold\")\n",
    "            plt.xlabel(col, fontsize=10)\n",
    "            plt.ylabel(\"Density\", fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_fig(filenaam)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# functie om het percentage outliers te berkenen voor een set van kolommen in een dataframe\n",
    "def bereken_percentage_aantal_outliers(df , columns_to_use):\n",
    "    # Initialiseren van het Isolation Forest model\n",
    "    iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "\n",
    "    # Fit het model\n",
    "    iso_forest.fit(df[columns_to_use])\n",
    "    # Voorspellingen\n",
    "    # Het geeft -1 voor outliers en 1 voor inliers\n",
    "    labels = iso_forest.predict(df[columns_to_use])\n",
    "    # Toevoegen van de labels aan het DataFrame om outliers te identificeren\n",
    "    df_intern = df.copy()\n",
    "    df_intern['outlier'] = labels\n",
    "    outliers = df_intern[df_intern['outlier'] == -1]\n",
    "    aantal_outliers = df_intern['outlier'].value_counts()\n",
    "    print(aantal_outliers)\n",
    "    percentage_aantal_outliers = (len(outliers) / len(df_intern)) * 100\n",
    "\n",
    "    return percentage_aantal_outliers\n",
    "\n",
    "\n",
    "# functie om outliers in een kolom te cappen op een percentiel waarde\n",
    "def cap_values(df_input, column, lower_percentile=25, upper_percentile=75):\n",
    "    # voeg code toe om beter de outliers te verwijderen\n",
    "    log(\"Info\", f\"Capping values voor kolom {column} naar lower percentiel {lower_percentile} - upper percentiel {upper_percentile}\")\n",
    "    q1, q3 = np.percentile(df_input[column], [lower_percentile, upper_percentile])  # Calculate the 25th (Q1) and 75th (Q3) percentiles\n",
    "    iqr = q3 - q1  # Calculate the interquartile range (IQR)\n",
    "    lower_bound = q1 - 1.5 * iqr  # Calculate lower whisker (Q1 - 1.5 * IQR)\n",
    "    upper_bound = q3 + 1.5 * iqr  # Calculate upper whisker (Q3 + 1.5 * IQR)\n",
    "\n",
    "    # lower_bound = df[column].quantile(lower_percentile)\n",
    "    # upper_bound = df[column].quantile(upper_percentile)\n",
    "    \n",
    "    # Waarden cappen met behulp van de numpy.where functie\n",
    "    df_output = df_input.copy()\n",
    "    df_output[column] = np.where(df_input[column] < lower_bound, lower_bound, df_input[column])\n",
    "    df_output[column] = np.where(df_input[column] > upper_bound, upper_bound, df_input[column])    \n",
    "    return df_output\n",
    "\n",
    "\n",
    "# hulp klasse om categorische waarden met twee mogelijke waarde naar 0 en 1 om te zetten\n",
    "# try-out om zelf eens een  encoder te schrijven\n",
    "# kan uiteraard eenvoudiger door OneHotEncoding toe te passen\n",
    "\n",
    "class BinaryValueEncoder(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, string_zero=\"nul\", string_one=\"een\"):\n",
    "        # Je kunt hier extra initialisatie toevoegen indien nodig\n",
    "        self.string_zero = string_zero\n",
    "        self.string_one = string_one\n",
    "     \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Er is geen fitting nodig voor deze eenvoudige codering\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X wordt aangenomen een pandas DataFrame te zijn\n",
    "        log(\"Info\", f\"BinaryValueEncoder transform opgeroepen voor One_value {self.string_one} en Zero_value {self.string_zero}\")\n",
    "        X = X.copy()  # Kopieer de DataFrame om wijzigingen te voorkomen in het origineel\n",
    "        X = X.applymap(lambda x: 1 if x == self.string_zero else 0)\n",
    "        return X\n",
    "\n",
    "\n",
    "def monkey_patch_get_signature_names_out():\n",
    "    \"\"\"Monkey patch some classes which did not handle get_feature_names_out()\n",
    "       correctly in Scikit-Learn 1.0.*.\"\"\"\n",
    "    from inspect import Signature, signature, Parameter\n",
    "    import pandas as pd\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.pipeline import make_pipeline, Pipeline\n",
    "    from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "    default_get_feature_names_out = StandardScaler.get_feature_names_out\n",
    "\n",
    "    if not hasattr(SimpleImputer, \"get_feature_names_out\"):\n",
    "      print(\"Monkey-patching SimpleImputer.get_feature_names_out()\")\n",
    "      SimpleImputer.get_feature_names_out = default_get_feature_names_out\n",
    "\n",
    "    if not hasattr(FunctionTransformer, \"get_feature_names_out\"):\n",
    "        print(\"Monkey-patching FunctionTransformer.get_feature_names_out()\")\n",
    "        orig_init = FunctionTransformer.__init__\n",
    "        orig_sig = signature(orig_init)\n",
    "\n",
    "        def __init__(*args, feature_names_out=None, **kwargs):\n",
    "            orig_sig.bind(*args, **kwargs)\n",
    "            orig_init(*args, **kwargs)\n",
    "            args[0].feature_names_out = feature_names_out\n",
    "\n",
    "        __init__.__signature__ = Signature(\n",
    "            list(signature(orig_init).parameters.values()) + [\n",
    "                Parameter(\"feature_names_out\", Parameter.KEYWORD_ONLY)])\n",
    "\n",
    "        def get_feature_names_out(self, names=None):\n",
    "            if callable(self.feature_names_out):\n",
    "                return self.feature_names_out(self, names)\n",
    "            assert self.feature_names_out == \"one-to-one\"\n",
    "            return default_get_feature_names_out(self, names)\n",
    "\n",
    "        FunctionTransformer.__init__ = __init__\n",
    "        FunctionTransformer.get_feature_names_out = get_feature_names_out\n",
    "\n",
    "\n",
    "    if not hasattr(BinaryValueEncoder, \"get_feature_names_out\"):\n",
    "        print(\"Monkey-patching FunctionTransformer.get_feature_names_out()\")\n",
    "        orig_init = BinaryValueEncoder.__init__\n",
    "        orig_sig = signature(orig_init)\n",
    "\n",
    "        def __init__(*args, feature_names_out=None, **kwargs):\n",
    "            orig_sig.bind(*args, **kwargs)\n",
    "            orig_init(*args, **kwargs)\n",
    "            args[0].feature_names_out = feature_names_out\n",
    "\n",
    "        __init__.__signature__ = Signature(\n",
    "            list(signature(orig_init).parameters.values()) + [\n",
    "                Parameter(\"feature_names_out\", Parameter.KEYWORD_ONLY)])\n",
    "\n",
    "        def get_feature_names_out(self, names=None):\n",
    "            if callable(self.feature_names_out):\n",
    "                return self.feature_names_out(self, names)\n",
    "            assert self.feature_names_out == \"one-to-one\"\n",
    "            return default_get_feature_names_out(self, names)\n",
    "\n",
    "        BinaryValueEncoder.__init__ = __init__\n",
    "        BinaryValueEncoder.get_feature_names_out = get_feature_names_out\n",
    "\n",
    "\n",
    "monkey_patch_get_signature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data bestand inlezen\n",
    "\n",
    "insurance_data_filename = 'data/insurance.csv'\n",
    "df = pd.read_csv(insurance_data_filename)\n",
    "log_info(f\"File {insurance_data_filename}\")\n",
    "\n",
    "# check op duplicates, indien zo verwijder direct\n",
    "df.drop_duplicates(inplace=True)\n",
    "df_original = df.copy()\n",
    "duplicate_waarden = df.duplicated().sum()\n",
    "log_info(f\"Check op duplicates na drop \\n{duplicate_waarden}\")\n",
    "\n",
    "# behoudt een copie van de orginele data\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enkele eenvoudige controles\n",
    "log_info(f\"df.info : \\n{df.info()}\")\n",
    "log_info(f\"df.describe : \\n{df.describe()}\")\n",
    "\n",
    "# geen nulwaarden \n",
    "from summarytools import dfSummary\n",
    "dfSummary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drie categorische features: smoking, region en sex\n",
    "# vier numerische features waaronder de target variabele \"charges\"\n",
    "# maak de datasets aan \n",
    "\n",
    "df_cat_col = ['smoker','region','sex']\n",
    "df_num_col = ['age', 'bmi','children']\n",
    "df_label_col = ['charges']\n",
    "\n",
    "# zijn er nominaal categorische variabelen en één ordinal categorische waarden?\n",
    "df_cat_nom_col = ['smoker','region']\n",
    "df_cat_ord_col = list(set(df_cat_col) - set(df_cat_nom_col))\n",
    "\n",
    "df_num = df[df_num_col]\n",
    "df_cat = df[df_cat_col]\n",
    "df_label = df[df_label_col]\n",
    "df_cat_nom = df[df_cat_nom_col]\n",
    "df_cat_ord = df[df_cat_ord_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korte analyze van de categorische variabelen\n",
    "print(df_cat.value_counts())\n",
    "\n",
    "# check op nullen\n",
    "nul_waarden = df.isnull().sum()\n",
    "log_info(f\"Check op nulwaarden \\n{nul_waarden}\")\n",
    "# geen nullen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(df, df_num_col,\"Boxplot en histogram van de numerische waarden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(df, df_label_col,\"Boxplot en histogram van de target value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_graphs:\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"bright\")\n",
    "    plt.figure(figsize=(15, len(df_cat_col) * 2))  \n",
    "\n",
    "    for i, col in enumerate(df_cat_col):\n",
    "        plt.subplot(1, len(df_cat_col), i + 1)\n",
    "        sns.countplot(x=col, data=df, palette='bright')\n",
    "        plt.title(f\"Count Plot of {col}\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.xlabel(col, fontsize=12)\n",
    "        plt.ylabel(\"Count\", fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"Categorische features countplot\")\n",
    "    plt.show()\n",
    "    log_info(\"Check van categorische features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_graphs:\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"bright\")\n",
    "    plt.figure(figsize=(15, len(df_cat_col) * 2))  \n",
    "\n",
    "    for i, col in enumerate(df_cat_col):\n",
    "        plt.subplot(1, len(df_cat_col), i + 1)\n",
    "        sns.countplot(x=col, data=df, palette='bright', hue='smoker')\n",
    "        plt.title(f\"Count Plot of {col} (Hue: Smoker)\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.xlabel(col, fontsize=12)\n",
    "        plt.ylabel(\"Count\", fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"Categorische variabelen tov smoker\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_graphs:\n",
    "    sns.pairplot(df, hue='smoker',  kind='reg')\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"Numerische features onderlinge scatter\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print de correlation nog af tussen de numerische waarden\n",
    "\n",
    "if plot_graphs:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"bright\")\n",
    "    sns.heatmap(df[df_num_col].corr(), annot=True, cmap='Reds')\n",
    "    save_fig(\"Numerische features correlatie\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verwijder_outliers(df_input, df_input_col):\n",
    "    # Bereken de outliers\n",
    "    percentage_aantal_outliers = bereken_percentage_aantal_outliers(df_input, df_input_col)\n",
    "    print(percentage_aantal_outliers)\n",
    "    log_info(f\"Check op de outliers in kolom {df_input_col} : {percentage_aantal_outliers}\")\n",
    "\n",
    "    df_output = df_input.copy()\n",
    "    for col in df_input_col:\n",
    "        df_output = cap_values(df_output, col)    \n",
    "\n",
    "    percentage_aantal_outliers = bereken_percentage_aantal_outliers(df_output, df_input_col)\n",
    "    print(percentage_aantal_outliers)\n",
    "    # plot_boxplot(df, df_num_col,\"Boxplot en histogram na removing van de outliers\")\n",
    "    log_info(f\"Check op de outliers in kolom [bmi, charges] na capping op kolom [bmi]  : {percentage_aantal_outliers}\")\n",
    "    log_info(\"Capping wordt niet toegepast op de dataset\")\n",
    "\n",
    "    return df_output\n",
    "\n",
    "plot_boxplot(df_original, df_num_col,\"Boxplot en histogram na removing van de outliers\")\n",
    "df_zonder_outliers = verwijder_outliers(df_original,['bmi'])\n",
    "plot_boxplot(df_zonder_outliers, df_num_col,\"Boxplot en histogram na removing van de outliers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preparation\n",
    "# standard scaler op de numerische waarden, min-max scaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])\n",
    "\n",
    "#set_matplotlib_closeV_encoder = BinaryValueEncoder(\"male\",\"female\")\n",
    "#set_matplotlib_smoking_encoder = BinaryValueEncoder(\"yes\",\"no\")\n",
    "# bv_encoder = PredefinedBinaryCategoricalEncoder(positive_class='female')\n",
    "\n",
    "male_female_transformer = Pipeline(steps=[\n",
    "     #('male_female_encoder', BinaryValueEncoder(\"male\",\"female\"))\n",
    "    ('male_female_encoder', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "smoking_transformer = Pipeline(steps=[\n",
    "    #('smoking_encoder', BinaryValueEncoder(\"yes\",\"no\"))\n",
    "    ('smoking_encoder', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "regio_transformer = Pipeline(steps=[\n",
    "    ('regio', OneHotEncoder(drop='first', handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, df_num_col),\n",
    "    (\"male_female\", male_female_transformer, ['sex']), \n",
    "    (\"smoker\", smoking_transformer, ['smoker']), \n",
    "    (\"regio\", regio_transformer, ['region'])],\n",
    "     remainder='passthrough')\n",
    "\n",
    "df_features = df_original.drop(['charges'], axis= 1)\n",
    "np_prepared =  preprocessing.fit_transform(df_features)\n",
    "\n",
    "# really hacking !!! get_feature_names werkt nog niet correct !!! not used now\n",
    "df_prepared_col = ['age', 'bmi','children','sex','smoker','northeast','northwest','southeast','southwest']\n",
    "\n",
    "df_prepared = pd.DataFrame(\n",
    "    np_prepared,\n",
    "    #columns=preprocessing.get_feature_names_out(),\n",
    "    columns = [name.split('__')[-1] for name in preprocessing.get_feature_names_out()],\n",
    "    #columns = df_prepared_col,\n",
    "    index=df_original.index)\n",
    "\n",
    "df_prepared.head()\n",
    "\n",
    "plot_boxplot(df_prepared,df_num_col,\"Boxplot van numerische waarden na standard scaling\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excute_regression_models(X_train, X_test, y_train,y_test):\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Lasso': Lasso(alpha=0.1) ,\n",
    "        'Ridge': Ridge(alpha=0.01) ,\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42), \n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42), \n",
    "        'XGBRegressor': XGBRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        # Cross validation\n",
    "        mean_rmse = np.sqrt(-cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean())\n",
    "        mean_r2 = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()      \n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        test_r2 = r2_score(y_test, y_pred)\n",
    "        test_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        results.append([name, mean_rmse, mean_r2, test_rmse, test_r2, test_mape])\n",
    "    df = pd.DataFrame(results, columns=['model', 'mean_rmse', 'mean_r2', 'test_rmse', 'test_r2', 'test_mape\"'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excute_regression_models_with_pipeline(X_train, X_test, y_train,y_test,preprocessor):\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Lasso': Lasso(alpha=0.1) ,\n",
    "        'Ridge': Ridge(alpha=0.01) ,\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42), \n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42), \n",
    "        'XGBRegressor': XGBRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('poly', PolynomialFeatures(degree=1, include_bias=False)),\n",
    "            ('regressor', model)\n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        # mean_rmse = np.sqrt(-cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean())\n",
    "        # mean_r2 = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()     \n",
    "\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        test_r2 = r2_score(y_test, y_pred)\n",
    "        test_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        results.append([name, test_rmse, test_r2, test_mape])\n",
    "    df = pd.DataFrame(results, columns=['model', 'test_rmse', 'test_r2', 'test_mape\"'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prepared.copy()\n",
    "y = df_original['charges']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "log_info(\"Train_test_split met stratefy = charges_cat\")\n",
    "\n",
    "resultaat_eenvoudig = excute_regression_models(X_train, X_test, y_train, y_test)\n",
    "print(resultaat_eenvoudig)\n",
    "log_info(\"Regresison models with no stratefy\")\n",
    "log_info(resultaat_eenvoudig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set opsplitsen\n",
    "# voorlopig via een aanvoudig train_test_split, nadien gaan we dit doen op het histogram van de charges\n",
    "\n",
    "\n",
    "X = df_prepared.copy()\n",
    "y = df_original['charges']\n",
    "\n",
    "df = df_original.copy()\n",
    "df[\"charges_cat\"] = pd.cut(df[\"charges\"], bins=[0, 20000,30000, np.inf], labels=[1, 2, 3])\n",
    "log_info(\"extra feature engineering: charges omzetten naar categories : bins=[0, 20000, 30000, np.inf\")\n",
    "df.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df[\"charges_cat\"] , test_size=0.2, random_state=42)\n",
    "log_info(\"Train_test_split met stratefy = charges_cat\")\n",
    "\n",
    "resultaat_eenvoudig = excute_regression_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(resultaat_eenvoudig)\n",
    "log_info(\"Regresison models with stratefy\")\n",
    "log_info(resultaat_eenvoudig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m10/02/2025 15:58:30 : Info : Train_test_split met stratefy = charges_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.302e+08, tolerance: 1.464e+07\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               model    test_rmse   test_r2  test_mape\"\n",
      "0  Linear Regression  4646.002397  0.882533    0.307566\n",
      "1              Lasso  4645.593546  0.882553    0.307523\n",
      "2              Ridge  4645.989784  0.882533    0.307589\n",
      "3      Random Forest  4823.857121  0.873367    0.403143\n",
      "4  Gradient Boosting  4368.847143  0.896130    0.312621\n",
      "5      Decision Tree  7066.079116  0.728284    0.511326\n",
      "6       XGBRegressor  5116.923498  0.857513    0.397980\n",
      "\u001b[0m10/02/2025 15:58:32 : Info : Regresison models with stratefy\n",
      "\u001b[0m10/02/2025 15:58:32 : Info :                model    test_rmse   test_r2  test_mape\"\n",
      "0  Linear Regression  4646.002397  0.882533    0.307566\n",
      "1              Lasso  4645.593546  0.882553    0.307523\n",
      "2              Ridge  4645.989784  0.882533    0.307589\n",
      "3      Random Forest  4823.857121  0.873367    0.403143\n",
      "4  Gradient Boosting  4368.847143  0.896130    0.312621\n",
      "5      Decision Tree  7066.079116  0.728284    0.511326\n",
      "6       XGBRegressor  5116.923498  0.857513    0.397980\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df_original.drop(['charges'], axis=1)\n",
    "y = df_original['charges']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "resultaat_eenvoudig = excute_regression_models_with_pipeline(X_train, X_test, y_train, y_test, preprocessing)\n",
    "\n",
    "print(resultaat_eenvoudig)\n",
    "log_info(\"\")\n",
    "log_info(\"Regression models with polynomial features\")\n",
    "log_info(resultaat_eenvoudig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m10/02/2025 16:01:32 : Info : extra feature engineering: charges omzetten naar categories : bins=[0, 20000, 30000, 50000, np.inf\n",
      "\u001b[0m10/02/2025 16:01:32 : Info : Train_test_split met stratefy en polynomial features = charges_cat\n",
      "               model    test_rmse   test_r2  test_mape\"\n",
      "0  Linear Regression  6528.468489  0.720580    0.433419\n",
      "1              Lasso  6528.395243  0.720587    0.433406\n",
      "2              Ridge  6528.458919  0.720581    0.433432\n",
      "3      Random Forest  5173.584333  0.824524    0.325311\n",
      "4  Gradient Boosting  4784.958831  0.849897    0.279509\n",
      "5      Decision Tree  6802.446036  0.696636    0.345666\n",
      "6       XGBRegressor  5380.070126  0.810238    0.355567\n",
      "\u001b[0m10/02/2025 16:01:33 : Info : Regresison models with stratefy\n",
      "\u001b[0m10/02/2025 16:01:33 : Info :                model    test_rmse   test_r2  test_mape\"\n",
      "0  Linear Regression  6528.468489  0.720580    0.433419\n",
      "1              Lasso  6528.395243  0.720587    0.433406\n",
      "2              Ridge  6528.458919  0.720581    0.433432\n",
      "3      Random Forest  5173.584333  0.824524    0.325311\n",
      "4  Gradient Boosting  4784.958831  0.849897    0.279509\n",
      "5      Decision Tree  6802.446036  0.696636    0.345666\n",
      "6       XGBRegressor  5380.070126  0.810238    0.355567\n"
     ]
    }
   ],
   "source": [
    "X = df_original.drop(['charges'], axis=1)\n",
    "y = df_original['charges']\n",
    "\n",
    "df = df_original.copy()\n",
    "df[\"charges_cat\"] = pd.cut(df[\"charges\"], bins=[0, 20000,30000, np.inf], labels=[1, 2, 3])\n",
    "log_info(\"extra feature engineering: charges omzetten naar categories : bins=[0, 20000, 30000, np.inf\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df[\"charges_cat\"] , test_size=0.2, random_state=42)\n",
    "resultaat_eenvoudig = excute_regression_models_with_pipeline(X_train, X_test, y_train, y_test, preprocessing)\n",
    "\n",
    "print(resultaat_eenvoudig)\n",
    "log_info(\"\")\n",
    "log_info(\"Regression models polynomial features en stratify on charges_cat\")\n",
    "log_info(resultaat_eenvoudig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpele linear regression \n",
    "\n",
    "linreg_model = LinearRegression()\n",
    "linreg_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "\n",
    "print(f\"We zitten er gemiddeld {mape*100:.2f}% naast met onze voorspellingen.\")\n",
    "log_info(f\"Linear regression met MSE {mse:.2f}, RMSE {rmse:.2f}, R² {r2:.2f}, gemiddelde afwijking {mape*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Perfect prediction line\n",
    "plt.title('Predicted vs Actual Charges')\n",
    "plt.xlabel('Actual Charges')\n",
    "plt.ylabel('Predicted Charges')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge Regression\n",
    "\n",
    "ridge = Ridge(alpha=0.01)  \n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = np.sqrt(mse_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"\\nRidge Regression Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_ridge}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge}\")\n",
    "print(f\"R-squared (R²): {r2_ridge}\")\n",
    "mape_ridge = mean_absolute_percentage_error(y_test, y_pred_ridge)\n",
    "print(f\"We zitten er gemiddeld {mape_ridge*100:.2f}% naast met onze voorspellingen.\")\n",
    "\n",
    "log_info(f\"Rdige regression met MSE {mse_ridge:.2f}, RMSE {rmse_ridge:.2f}, R² {r2_ridge:.2f}, gemiddelde afwijking {mape_ridge*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_ridge, color='blue', alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Perfect prediction line\n",
    "plt.title('Predicted vs Actual Charges')\n",
    "plt.xlabel('Actual Charges')\n",
    "plt.ylabel('Predicted Charges')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.1)  \n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "rmse_lasso = np.sqrt(mse_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "mape_lasso = mean_absolute_percentage_error(y_test, y_pred_lasso)\n",
    "print(f\"We zitten er gemiddeld {mape_lasso*100:.2f}% naast met onze voorspellingen.\")\n",
    "\n",
    "print(\"Lasso Regression Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lasso}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lasso}\")\n",
    "print(f\"R-squared (R²): {r2_lasso}\")\n",
    "log_info(f\"Lasso regression met MSE {mse_lasso:.2f}, RMSE {rmse_lasso:.2f}, R² {r2_lasso:.2f}, gemiddelde afwijking {mape_lasso*100:.2f}%\")\n",
    "log_info_write_to_file(\"LinReg_logging.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_original.drop(columns=['charges'])\n",
    "y = df_original['charges']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso': Lasso(alpha=0.1) ,\n",
    "    'Ridge': Ridge(alpha=0.01) ,\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42), \n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42), \n",
    "    'XGBRegressor': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    # Cross validation\n",
    "    mean_rmse = np.sqrt(-cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean())\n",
    "    mean_r2 = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    results.append([name, mean_rmse, mean_r2, test_rmse, test_r2])\n",
    "df = pd.DataFrame(results, columns=['model', 'mean_rmse', 'mean_r2', 'test_rmse', 'test_r2'])\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
